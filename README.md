# Trump and Biden for President Campaigns
### collection of PDF documents, such as contracts, orders, invoices and other documents
This repository is a demo of processing a collection of historical PDF documents of various types with a view to save manual hours of potentially highly skilled personnel. 

# Table of Contents
- [Trump and Biden for President Campaigns](#trump-and-biden-for-president-campaigns)
    - [collection of PDF documents, such as contracts, orders, invoices and other documents](#collection-of-pdf-documents-such-as-contracts-orders-invoices-and-other-documents)
- [Table of Contents](#table-of-contents)
  - [Introduction](#introduction)
  - [Main Processing Tasks](#main-processing-tasks)
    - [Download, Pre-process and Store PDFs](#download-pre-process-and-store-pdfs)
    - [Visualization of PDFs](#visualization-of-pdfs)
    - [Classification of PDFs](#classification-of-pdfs)
      - [Zero-shot Classification](#zero-shot-classification)
      - [Analyzing discrepancies](#analyzing-discrepancies)
      - [Dataset for Fine-tuning](#dataset-for-fine-tuning)
      - [Fine-tuning/Quantization](#fine-tuningquantization)
    - [Creating Vector Embeddings](#creating-vector-embeddings)

## Introduction

In the 1980s and early 1990s, the landscape of word processors was diverse and fragmented. Popular names included WordPerfect, Microsoft Word, WordStar, AppleWorks (later Claris Works), among others. Their primary function was to create text files, devoid of images. However, these editors were incompatible with each other – a file created in one editor could only be opened with the same software installed on your PC. Moreover, each text editor required a separate purchase, and compatibility was limited to printer output.

During this era, there was a pressing need for software that could enable on-screen visualization of documents generated by various text editors. In 1991, John Warnock, the inventor of the Portable Document Format (PDF), articulated this need: 

“What industries badly need is a universal way to communicate documents across a wide variety of machine configurations, operating systems and communication networks. These documents should be viewable on any display and should be printable on any modern printers. If this problem can be solved, then the fundamental way people work will change.”

Since its inception, PDF has arguably become the most ubiquitous document format globally. It is challenging to quantify the vast amounts of data stored in laptops, PCs, PDAs, websites, and other digital mediums worldwide. This data, while semi-structured, presents a structure defined by elements like text coordinates and fonts. However, this structure is designed for human readability and is not readily transformable into a structured format suitable for SQL databases. 

This repository aims to address these challenges by providing tools and solutions for extracting and transforming data from PDF files into more structured and usable formats, bridging the gap between human-readable and machine-readable data.

We will be using a collection of PDF documents, such as contracts, orders, invoices, and other documents related to TV ads from Trump and Biden for President Campaigns, which they are required to disclose to the Federal Communications Commission (FCC) under the Communications Act of 1934. We will use this collection of PDF documents to demonstrate the process of extracting and transforming data from PDF files. The repository will cover the following main processing tasks:

## Main Processing Tasks
### Download, Pre-process and Store PDFs
<a target="_blank" href=https://colab.research.google.com/github/aguille-vert/trump-biden-ads/blob/main/notebooks/trump_biden_download_preprocess_store.ipynb><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>   
The first step in the process is to download the PDF files using [FCC API](https://www.google.com/url?q=https%3A%2F%2Fpublicfiles.fcc.gov%2Fdeveloper), preprocess and store them in the following 3 formats:
* original pdf files (.pdf)
* page-by-page image files (.jpg)
* extracted text organized as json files (.json)
. The preprocessing step involves extracting text from the PDFs, removing any unwanted characters, and storing the text in a structured format. The structured format can be a CSV file, a SQL database, or any other format that is suitable for further analysis.

### Visualization of PDFs
[trump_biden demo app](https://trump-biden.streamlit.app/)  

The second step is to visualize the PDF files using the extracted images and text. For this, we will use the [Streamlit](https://streamlit.io) library. Streamlit is an open-source app framework for Machine Learning and Data Science projects. It allows you to create beautiful, interactive web apps for your projects with minimal effort. We will create a Streamlit app that displays the PDF files page by page. The app will allow users to search for specific text, initially in the titles of the PDFs. Once we have implemented the Retrieval of Values task and created vector embeddings, we will build a powerful semantic search engine turbocharged with the RAG (Retrieval Augmented Generation).

### Classification of PDFs
In the PDF collection which we made there are files of different types: contracts, invoices, orders, etc. We don't know the types of the files in advance. However, for further processing, we need to classify the files into different categories. 
This is a multi-class classification problem. 
#### Zero-shot Classification
We will first use 2 zero-shot open-source LLMs to for the initial classification of the PDFs:
* llama3-70b-8192
* gemma-7b-it  
we will use [groq cloud API](https://groq.cloud), since it is the lowest-cost API for this task.  
To save costs and energy, we will use only the first page of each file for the classification.
#### Analyzing discrepancies
We will analyze the discrepancies between the two models, assuming that the correct classification is the one that is agreed upon by both models. We will see that there are too many discrepancies, which means that zero-shot classification is not enough for this task. We will need to fine-tune a model on a labeled dataset.
#### Dataset for Fine-tuning
<a target="_blank" href=https://colab.research.google.com/github/aguille-vert/trump-biden-ads/blob/main/notebooks/inference_analysis_dataset.ipynb><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a> 
We will then use this information to create a labeled dataset for fine-tuning a model. We will create 2 datasets: one for superfised fine-tuning, the second one for fine-tuning with [DPO objective](https://arxiv.org/abs/2305.18290)
#### Fine-tuning/Quantization
Our goal is to find the best balance between accuracy and computational cost.  
We will experiment with different models and fine-tuning techniques to classify the PDFs:
* [LoRA](https://arxiv.org/abs/2106.09685)
* [ReFT](https://arxiv.org/abs/2404.03592)
* both with [DPO objective](https://arxiv.org/abs/2305.18290)

### Creating Vector Embeddings
...




